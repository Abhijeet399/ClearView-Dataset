{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import noisy images and masked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv, concat and outputting latent vector \n",
    "def encode(z,z_bw):\n",
    "    filters = {1:20,2:20,3:60, 4:60, 5:60, 6:60}\n",
    "    conv1 = tf.layers.conv2d(z, filters[1], (3,3),strides=(2, 2), padding='valid',activation=None, use_bias=True, kernel_initializer='random_uniform', bias_initializer=tf.zeros_initializer())\n",
    "    conv2 = tf.layers.conv2d(conv1, filters[2], (3,3),strides=(2, 2), padding='valid',activation=None, use_bias=True, kernel_initializer='random_uniform', bias_initializer=tf.zeros_initializer())\n",
    "    conv1_bw = tf.layers.conv2d(z_bw, filters[1], (3,3),strides=(2,2),padding='valid',activation=None,use_bias=True, kernel_initializer='random_uniform', bias_initializer=tf.zeros_initializer())\n",
    "    conv2_bw = tf.layers.conv2d(conv1_bw, filters[2], (3,3),strides=(2,2),padding='valid',activation=None,use_bias=True, kernel_initializer='random_uniform', bias_initializer=tf.zeros_initializer())\n",
    "\n",
    "    conv = tf.concat([conv2, conv2_bw], 3)\n",
    "    conv3 = tf.layers.conv2d(conv, filters[3], (3,3),strides=(2, 2), padding='valid',activation=None, use_bias=True, kernel_initializer='random_uniform', bias_initializer=tf.zeros_initializer())\n",
    "    conv4 = tf.layers.conv2d(conv3, filters[4], (3,3),strides=(2, 2), padding='valid',activation=None, use_bias=True, kernel_initializer='random_uniform', bias_initializer=tf.zeros_initializer())\n",
    "     \n",
    "    latent_variable = conv\n",
    "\n",
    "    return latent_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for deconv\n",
    "def decode(x):\n",
    "    act = tf.nn.relu(x)\n",
    "    upsample1 = tf.image.resize_nearest_neighbor(act, (512,512))\n",
    "    conv5 = tf.layers.conv2d(upsample1, 40, (7,7), padding='same',activation=tf.nn.relu)\n",
    "    upsample2 = tf.image.resize_nearest_neighbor(conv5, (636,636))\n",
    "    conv6 = tf.layers.conv2d(upsample2, 20, (5,5), padding='same',activation=tf.nn.relu)\n",
    "    upsample3 = tf.image.resize_nearest_neighbor(conv6, (776,776))\n",
    "    conv7 = tf.layers.conv2d(upsample3, 3, (7,7), padding='same',activation=tf.nn.relu)\n",
    "    upsample4 = tf.image.resize_nearest_neighbor(conv7, (1024,1024))\n",
    "    output = tf.layers.conv2d(upsample4, 3, (7,7), padding='same',activation=tf.nn.relu)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ground truth/ noise free images and their latent distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving reconst images and ground truth images to discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    # if input tensor is a 3D array of size Nh x Nw X Nc\n",
    "    # we reshape it to a 2D array of Nc x (Nh*Nw)\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "\n",
    "    # get gram matrix \n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "\n",
    "    return gram\n",
    "\n",
    "def get_style_loss(base_style, gram_target):\n",
    "    height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "    # Original eqn as a constant to divide i.e 1/(4. * (channels ** 2) * (width * height) ** 2)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target)) / (channels**2 * width * height) #(4.0 * (channels ** 2) * (width * height) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruction loss\n",
    "def reconstruction_loss(x_target, reconstructed_image):\n",
    "    lr=0.001\n",
    "    L2_loss = tf.reduce_mean(tf.square(x_target - reconstructed_image))\n",
    "    style_loss = get_style_loss(reconstructed_image, x_target)\n",
    "    reconstruction_loss = L2_loss + style_loss\n",
    "    reconstruction_optimizer = tf.train.AdamOptimizer(learning_rate=lr,\n",
    "                                                       beta1=0.9).minimize(reconstruction_loss)\n",
    "    return reconstruction_loss, reconstruction_optimizer\n",
    "\n",
    "#KL divergence loss\n",
    "def KLdivergence_loss(encode_LD, groundtruth_LD):\n",
    "    k = k = tf.keras.losses.KLDivergence()\n",
    "    kl_loss = k(encode_LD, groundtruth_LD)\n",
    "    return kl_loss\n",
    "\n",
    "#wgan loss\n",
    "def wgan_loss(d_real, d_fake):\n",
    "    lr=0.01\n",
    "    disc_loss = tf.reduce_mean(d_real)-tf.reduce_mean(d_fake)\n",
    "    discriminator_optimizer = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(-disc_loss)\n",
    "    return disc_loss, discriminator_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputing the reconstructed image\n",
    "encode_out = encode(X, X_bw)\n",
    "reconstructed_image = decode(encode_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst_loss, reconst_opt = reconstruction_loss(X_target, reconstructed_image)\n",
    "kl_loss = KLdivergence_loss(encode_out, gt_LD)\n",
    "disc_loss, disc_opt = wgan_loss(X_target, reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
