{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "# Placeholders for input data and the targets\n",
    "X_input = tf.placeholder(tf.float32, (None, img_size, img_size, 3), name='Input')\n",
    "X_input_mask = tf.placeholder(tf.float32, (None, img_size, img_size, 1), name='Input_bw')\n",
    "X_latent_vector = tf.placeholder(tf.float32, (None, img_size, img_size, 1), name='Input_bw')\n",
    "X_target = tf.placeholder(tf.float32, (None, img_size, img_size, 3), name='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16_RGB:\n",
    "    def __init__(self, imgs, weights=None, sess=None):\n",
    "        self.imgs = imgs\n",
    "        self.convlayers()\n",
    "#         self.fc_layers()\n",
    "        self.map = self.pool5\n",
    "#         self.probs = tf.nn.softmax(self.fc3l)\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "\n",
    "\n",
    "    def convlayers(self):\n",
    "        self.parameters = []\n",
    "\n",
    "        # zero-mean input\n",
    "        with tf.name_scope('preprocess') as scope:\n",
    "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "            images = self.imgs-mean\n",
    "\n",
    "        # conv1_1\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv1_2\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool1')\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.name_scope('conv5_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        with tf.name_scope('conv5_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        with tf.name_scope('conv5_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "#     def fc_layers(self):\n",
    "#         # fc1\n",
    "#         with tf.name_scope('fc1') as scope:\n",
    "#             shape = int(np.prod(self.pool5.get_shape()[1:]))\n",
    "#             fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "#                                                          dtype=tf.float32,\n",
    "#                                                          stddev=1e-1), name='weights')\n",
    "#             fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "#                                  trainable=True, name='biases')\n",
    "#             pool5_flat = tf.reshape(self.pool5, [-1, shape])\n",
    "#             fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "#             self.fc1 = tf.nn.relu(fc1l)\n",
    "#             self.parameters += [fc1w, fc1b]\n",
    "\n",
    "#         # fc2\n",
    "#         with tf.name_scope('fc2') as scope:\n",
    "#             fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "#                                                          dtype=tf.float32,\n",
    "#                                                          stddev=1e-1), name='weights')\n",
    "#             fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "#                                  trainable=True, name='biases')\n",
    "#             fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n",
    "#             self.fc2 = tf.nn.relu(fc2l)\n",
    "#             self.parameters += [fc2w, fc2b]\n",
    "\n",
    "#         # fc3\n",
    "#         with tf.name_scope('fc3') as scope:\n",
    "#             fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "#                                                          dtype=tf.float32,\n",
    "#                                                          stddev=1e-1), name='weights')\n",
    "#             fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n",
    "#                                  trainable=True, name='biases')\n",
    "#             self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n",
    "#             self.parameters += [fc3w, fc3b]\n",
    "\n",
    "    def load_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            print(i, k, np.shape(weights[k]))\n",
    "            sess.run(self.parameters[i].assign(weights[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16_Mask:\n",
    "    def __init__(self, imgs, weights=None, sess=None):\n",
    "        self.imgs = imgs\n",
    "        self.convlayers()\n",
    "#         self.fc_layers()\n",
    "        self.map = self.pool5\n",
    "#         self.probs = tf.nn.softmax(self.fc3l)\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "\n",
    "\n",
    "    def convlayers(self):\n",
    "        self.parameters = []\n",
    "\n",
    "        # zero-mean input\n",
    "        with tf.name_scope('preprocess') as scope:\n",
    "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "            images = self.imgs-mean\n",
    "\n",
    "        # conv1_1\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            #[3, 3, 3, 64] represents [3, 3] filter size and 3 input channels, 64 output channels\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv1_2\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool1')\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.name_scope('conv5_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        with tf.name_scope('conv5_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        with tf.name_scope('conv5_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "#     def fc_layers(self):\n",
    "#         # fc1\n",
    "#         with tf.name_scope('fc1') as scope:\n",
    "#             shape = int(np.prod(self.pool5.get_shape()[1:]))\n",
    "#             fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "#                                                          dtype=tf.float32,\n",
    "#                                                          stddev=1e-1), name='weights')\n",
    "#             fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "#                                  trainable=True, name='biases')\n",
    "#             pool5_flat = tf.reshape(self.pool5, [-1, shape])\n",
    "#             fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "#             self.fc1 = tf.nn.relu(fc1l)\n",
    "#             self.parameters += [fc1w, fc1b]\n",
    "\n",
    "#         # fc2\n",
    "#         with tf.name_scope('fc2') as scope:\n",
    "#             fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "#                                                          dtype=tf.float32,\n",
    "#                                                          stddev=1e-1), name='weights')\n",
    "#             fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "#                                  trainable=True, name='biases')\n",
    "#             fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n",
    "#             self.fc2 = tf.nn.relu(fc2l)\n",
    "#             self.parameters += [fc2w, fc2b]\n",
    "\n",
    "#         # fc3\n",
    "#         with tf.name_scope('fc3') as scope:\n",
    "#             fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "#                                                          dtype=tf.float32,\n",
    "#                                                          stddev=1e-1), name='weights')\n",
    "#             fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n",
    "#                                  trainable=True, name='biases')\n",
    "#             self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n",
    "#             self.parameters += [fc3w, fc3b]\n",
    "\n",
    "    def load_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            print(i, k, np.shape(weights[k]))\n",
    "            sess.run(self.parameters[i].assign(weights[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(z, reuse=False):\n",
    "    \"\"\"\n",
    "    Encode part of the autoencoder.\n",
    "    :param x: input to the autoencoder\n",
    "    :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which is the hidden latent variable of the autoencoder.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Encoder'):\n",
    "        filters = {1:2,2:1,3:3, 4:40, 5:50, 6:60}\n",
    "        conv1 = tf.layers.conv2d(z, filters[1], (3,3),strides=(1, 1), padding='valid',activation=tf.nn.relu, use_bias=True, kernel_initializer='random_uniform', bias_initializer=tf.zeros_initializer())\n",
    "        conv2 = tf.layers.conv2d(conv1, filters[1], (3,3),strides=(1,1),padding='valid',activation=tf.nn.relu,use_bias=True, kernel_initializer='random_uniform', bias_initializer=tf.zeros_initializer())\n",
    "        \n",
    "#         conv = tf.concat([conv3, conv4], 3)\n",
    "        #conv_shortcut = conv\n",
    "        conv3 = tf.layers.conv2d(con2, filters[3],(3,3), strides=(1, 1), padding='valid', activation=tf.nn.relu,use_bias=True, kernel_initializer='random_uniform', bias_initializer=tf.zeros_initializer())\n",
    "        print('conv1',np.shape(conv1))\n",
    "        print('conv2',np.shape(conv2))\n",
    "        print('conv3',np.shape(conv3))\n",
    "        print('conv4',np.shape(conv4))\n",
    "        print('conv5',np.shape(conv5))\n",
    "        lv = tf.contrib.layers.flatten(conv5)\n",
    "    return conv3, lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    act = tf.nn.relu(x)\n",
    "    upsample1 = tf.image.resize_nearest_neighbor(act, (512,512))\n",
    "    conv4 = tf.layers.conv2d(upsample1, 512, (7,7), padding='same',activation=tf.nn.relu)\n",
    "    upsample2 = tf.image.resize_nearest_neighbor(conv5, (636,636))\n",
    "    conv5 = tf.layers.conv2d(upsample2, 256, (5,5), padding='same',activation=tf.nn.relu)\n",
    "    upsample3 = tf.image.resize_nearest_neighbor(conv6, (776,776))\n",
    "    conv6 = tf.layers.conv2d(upsample3, 128, (7,7), padding='same',activation=tf.nn.relu)\n",
    "    upsample4 = tf.image.resize_nearest_neighbor(conv7, (1024,1024))\n",
    "    conv7 = tf.layers.conv2d(upsample4, 64, (7,7), padding='same',activation=tf.nn.relu)\n",
    "    upsample1 = tf.image.resize_nearest_neighbor(act, (512,512))\n",
    "    conv8 = tf.layers.conv2d(upsample1, 16, (7,7), padding='same',activation=tf.nn.relu)\n",
    "    upsample2 = tf.image.resize_nearest_neighbor(conv5, (636,636))\n",
    "    conv9 = tf.layers.conv2d(upsample2, 3, (5,5), padding='same',activation=tf.nn.relu)\n",
    "    return conv9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X,reuse=None):\n",
    "    with tf.variable_scope('dis',reuse=reuse):\n",
    "        conv = tf.layers.conv2d(X, 20, (5,5), padding='same',activation=tf.nn.relu)\n",
    "        features = tf.contrib.layers.flatten(latent_vector_map)\n",
    "        hidden1=tf.layers.dense(inputs=features,units=512,activation=tf.nn.leaky_relu)\n",
    "        hidden2=tf.layers.dense(inputs=hidden1,units=256,activation=tf.nn.leaky_relu)\n",
    "        hidden3=tf.layers.dense(inputs=hidden2,units=128,activation=tf.nn.leaky_relu)\n",
    "        logits=tf.layers.dense(hidden3,units=1)\n",
    "        output=tf.sigmoid(logits)\n",
    "        \n",
    "        return output,logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_normal_distribution(input_shape):\n",
    "    mu, sigma = 0.0, 1.0\n",
    "    norm_dist = np.random.normal(mu, sigma, input_shape)\n",
    "    count, bins, ignored = plt.hist(norm_dist, 20, normed=True)\n",
    "    return norm_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    # if input tensor is a 3D array of size Nh x Nw X Nc\n",
    "    # we reshape it to a 2D array of Nc x (Nh*Nw)\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "\n",
    "    # get gram matrix \n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "\n",
    "    return gram\n",
    "\n",
    "def get_style_loss(base_style, gram_target):\n",
    "    height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "    # Original eqn as a constant to divide i.e 1/(4. * (channels ** 2) * (width * height) ** 2)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target)) / (channels**2 * width * height) #(4.0 * (channels ** 2) * (width * height) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x_target, reconstructed_image):\n",
    "    #lr=0.001\n",
    "    L2_loss = tf.reduce_mean(tf.square(x_target - reconstructed_image))\n",
    "    style_loss = get_style_loss(reconstructed_image, x_target)\n",
    "    reconstruction_loss = L2_loss \n",
    "    #reconstruction_optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9).minimize(reconstruction_loss)\n",
    "    return reconstruction_loss, style_loss\n",
    "\n",
    "#KL divergence loss\n",
    "def KLdivergence_loss(encode_LD, groundtruth_LD, normal_dist):\n",
    "    k = tf.keras.losses.KLDivergence()\n",
    "    kl_loss = k(encode_LD, groundtruth_LD) + k(encode_LD, normal_dist) + k(groundtruth_LD, normal_dist)\n",
    "    return tf.reduce_mean(kl_loss)\n",
    "\n",
    "#wgan loss\n",
    "def wgan_loss(d_real, d_fake):\n",
    "    #lr=0.01\n",
    "    disc_loss = tf.reduce_mean(d_real) - tf.reduce_mean(d_fake)\n",
    "    #discriminator_optimizer = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(-disc_loss)\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_dependent_loss(reconstructed_image, groundtruth_image, count):\n",
    "    arg_list = open()\n",
    "    w, h = arg_list(count)\n",
    "    \n",
    "    #crop reconst_image to get reconst_patch\n",
    "    #crop gt_image to get gt_patch\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            if (i<j):\n",
    "                gamma[i][j] = (0.99)**(i)\n",
    "            else:\n",
    "                gamma[i][j] = (0.99)**(j)\n",
    "                \n",
    "    L2_loss = (tf.square(reconstructedpatch - groundtruthpatch))\n",
    "    distance_dependent_loss = tf.reduce_mean(gamma * L2_loss)\n",
    "    return distance_dependent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(i):\n",
    "    training_label = []\n",
    "    training_mask = []\n",
    "    training_image = []\n",
    "    for j in range((i*62),(62+(i*62))):\n",
    "        image = cv2.imread('D:\\Temp\\data_256\\h\\highway/00000%d.jpg' %(j+100))\n",
    "        training_label.append(image)\n",
    "        x = y = randint(100, 110)\n",
    "        w = h = randint(60, 65)\n",
    "        print(x,y,w,h)\n",
    "        image[x:(x+w),y:(y+h)] = 255\n",
    "        training_image.append(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY ) \n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                if (image[i][j] >= 128):\n",
    "                    image[i][j] = 255\n",
    "                else:\n",
    "                    image[i][j] = 0\n",
    "        training_mask.append(image)\n",
    "    return training_image, training_mask, training_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\bhatt\\desktop\\jupyter notebook\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-1eb1dfaaf490>\", line 2, in <module>\n",
      "    sess = tf.Session()\n",
      "NameError: name 'tf' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\bhatt\\desktop\\jupyter notebook\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\bhatt\\desktop\\jupyter notebook\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\bhatt\\desktop\\jupyter notebook\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\bhatt\\desktop\\jupyter notebook\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Python36\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Python36\\lib\\inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Python36\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Python36\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Python36\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Python36\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Python36\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sess = tf.Session()\n",
    "    vgg_RGB = vgg16_RGB(X_input, 'vgg16_weights.npz', sess)\n",
    "    vgg_Mask = vgg16_Mask(X_input_mask, 'vgg16_weights.npz', sess)\n",
    "    conv_RGB = vgg16_RGB.pool5\n",
    "    conv_Mask = vgg16_Mask.pool5\n",
    "    conv_concat = tf.concat([conv_RGB, conv_Mask], 3)\n",
    "    latent_vector_map = encoder(conv_concat)\n",
    "    latent_vector_feature = tf.contrib.layers.flatten(latent_vector_map)\n",
    "    reconstructed_image = decode(latent_vector_map)\n",
    "    norm_dist = make_normal_distribution(latent_vector_feature.get_shape())\n",
    "    \n",
    "    lambda_1 = 0.9\n",
    "    lambda_2 = 0.8\n",
    "    lambda_3 = 0.85\n",
    "    lambda_4 = 0.88\n",
    "    \n",
    "    # LOSSES\n",
    "    \n",
    "    D_output_real,D_logits_real=discriminator(X_target)\n",
    "    \n",
    "    D_output_fake,D_logits_fake=discriminator(reconstructed_image,reuse=True)\n",
    "    \n",
    "    w_gan_loss = wgan_loss(D_output_real, D_output_fake)\n",
    "    \n",
    "    reconstruction_loss, style_loss = reconstruction_loss(X_target, reconstructed_image)\n",
    "    \n",
    "    discounted_reconstruction_loss = distance_dependent_loss(reconstructed_image, X_target, counter)\n",
    "    \n",
    "    KLdivergence_loss = KLdivergence_loss(latent_vector_feature, X_latent_vector, norm_dist)\n",
    "    \n",
    "    total_loss = (lambda_1*reconstruction_loss) + (lambda_2*KLdivergence_loss) + (lambda_3*style_loss) - (lambda_4*w_gan_loss)\n",
    "    \n",
    "    reconstruction_optimizer = tf.train.AdamOptimizer(learning_rate=0.001,\n",
    "                                                       beta1=0.9).minimize(total_loss)\n",
    "    \n",
    "    batch_size=62\n",
    "    epochs=100\n",
    "    num_batches=56\n",
    "    init=tf.global_variables_initializer()\n",
    "    samples_n = [] \n",
    "    minibatch_loss_n = 0\n",
    "    cost = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            count=0\n",
    "            for i in range(num_batches):\n",
    "                X, X_mask, Y = load_dataset(i)\n",
    "                _, temp_loss = sess.run([reconstruction_optimizer,total_loss, count], feed_dict={X_input:X, X_input_mask:X_mask, X_target:Y, })\n",
    "#                 _, temp_loss = sess.run([disc_opt,disc_loss], feed_dict={x_input:X, x_input_bw:X_bw, x_input_mask:X_mask})\n",
    "                minibatch_loss_n += temp_loss / num_batches\n",
    "                count+=1\n",
    "        \n",
    "            print(\"on epoch{}\".format(epoch))\n",
    "            # Print the cost every epoch\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, minibatch_loss_n))\n",
    "            cost_n.append(minibatch_loss_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
